---
title: "05_Deploy_classifier"
author: "Victor Yuan"
date: "November 14, 2018"
output:
  html_document:
    keep_md: true
    toc: true
    toc_float: true
    toc_depth: 2
editor_options: 
  chunk_output_type: console
---

This script is for extracting the final model and saving it for publication. To make this tool more 
user-friendly, I create a function that wraps the prediction to output the following variables:

* Probabilities that a sample belongs to a specific class (1 for each ethnicity)
* A class label determined by the highest class-specific probability
* A class label determined after applying a user-supplied threshold function for 'ambiguous' samples
* The highest class-specific probability, used to determine the threshold

# Libraries and data

```{r}
library(dplyr)
library(impute)
library(caret)
library(glmnet)
library(broom)

# Load model and data
GLM_cv <- readRDS('../../Robjects_final/02_GLM_cv_logloss.rds')
betas <- readRDS('../../Robjects_final/01_processed_betas_EPIC.rds') 
dim(betas) #  319625    510
pDat <- readRDS('../../Robjects_final/01_pDat.rds')
dim(pDat) # 510 22

# knn impute
sum(is.na(betas))
set.seed(1)
betas <- impute.knn(as.matrix(betas), maxp = 15000)$data

#subset out south asians
betas_SA <- betas[,which(pDat$Ethnicity == 'South_Asian')]
pDat_SA <- pDat[which(pDat$Ethnicity == 'South_Asian'),]
dim(betas_SA);dim(pDat_SA) # 7 samples

glm_fit <- GLM_cv$finalModel
features <- glm_fit$beta$African@Dimnames[[1]]
```

Here I manually create the outputs mentioned above. Later I implement a function that automatically
does this.

# Infer ethnicity 

```{r}
# obtain predictions
preds <- as.data.frame(predict(glm_fit, t(betas_SA), s = glm_fit$lambdaOpt, type = 'class'))
probs <- as.data.frame(predict(glm_fit, t(betas_SA), s = glm_fit$lambdaOpt, type = 'response'))

#combine
pred_prob <- cbind(preds, probs)
colnames(pred_prob) <- c('Predicted_ethnicity_nothresh', paste0('Prob_', glm_fit$classnames))

# calculate highest probability
pred_prob$Highest_Prob <- apply(pred_prob[,2:4], 1, max)

# call ambiguous if below threshold
pred_prob$Predicted_ethnicity <- ifelse(pred_prob$Highest_Prob < 0.75, 'Ambiguous', 
                                        as.character(pred_prob$Predicted_ethnicity_nothresh))
pred_prob$Sample_ID <- rownames(pred_prob)
pred_prob <- pred_prob[,c(7,1,6, 2:5)]
```

# Function: Attempt 1

Now I wrap the above code into a function:

```{r}
pl_infer_ethnicity <- function(betas, threshold = 0.75){ # betas need to be in the form of samples in columns
  if(!all(features %in% rownames(betas))) {
    stop('Rownames of betas df must include all 319625 features used to fit classifier.')
  }
  
  betas <- t(betas[features,])
  
  # obtain predictions
  preds <- as.data.frame(predict(glm_fit, betas, s = glm_fit$lambdaOpt, type = 'class'))
  probs <- as.data.frame(predict(glm_fit, betas, s = glm_fit$lambdaOpt, type = 'response'))
  p <- cbind(preds, probs)
  colnames(p) <- c('Predicted_ethnicity_nothresh', paste0('Prob_', glm_fit$classnames))

  p$Highest_Prob <- apply(p[,2:4], 1, max)
  
  p$Predicted_ethnicity <- ifelse(p$Highest_Prob < threshold, 'Ambiguous', 
                                        as.character(p$Predicted_ethnicity_nothresh))
  p$Sample_ID <- rownames(p)
  p <- p[,c(7,1,6, 2:5)]
  
  return(p)
}

pl_infer_ethnicity(betas_SA, threshold = 0.75)
```

Unfortunately the above code requires that new samples have all the features used for training 
(n=319625), when only 1862 are required for the final prediction. Below I extract the coefficients
from the final model and see if I can create the same predictions using a cross product with the 
sample vector.

# Function: Attempt 2

```{r eval = F}
# extract coefficients
coef <- coef(glm_fit, glm_fit$lambdaOpt)

# bind feature coefficients for each cpg and intercept
out <- do.call("cbind", lapply(coef, function(x) x[,1])) 
out <- as.data.frame(out) %>% mutate(feature = rownames(out)) %>% as_tibble()
out$feature[1] <- 'Intercept'

# should be 1862 + 1 intercept
out <- out %>% filter(abs(African) > 0 | abs(Asian) > 0 | abs(Caucasian) > 0)
out %>% arrange(desc(Asian), desc(African), desc(Caucasian))

#filter data to features
newDat <- betas_SA[out$feature[2:nrow(out)],2]

#cross product, adding 1 for intercept term
af <- out$African %*% c(1, newDat)
as <- out$Asian %*% c(1, newDat)
ca <- out$Caucasian %*% c(1, newDat)

af/sum(af,as,ca)
as/sum(af,as,ca)
ca/sum(af,as,ca)
```

After taking out the coefficients and trying the cross product, I get values that I can't make sense
of. I think I need to implement a loglink function on this output, but I'm not sure how to do this.

Instead, I create a workaround, where given a new samples with the final 1862 features, I add on
'fake' data of the remaining 319625-1862 features so that the predict() function accepts it.

# Function: Attempt 3

```{r}
# create 'new' data of only the necessary features
pl_features <- predictors(GLM_cv)
newDat <- betas_SA[pl_features,]
dim(newDat) #1862 features, 7 samples

pl_infer_ethnicity <- function(betas, threshold = 0.75) {
  
  # find all final predictors in new data
  present_features <- intersect(rownames(betas), pl_features)
  print(paste(length(present_features), 'of 1862 predictors present.'))
  
  dat <- betas[present_features,]
  dim(dat)
  
  # find missing non-predictors used for training
  train_features <- glm_fit$beta$African@Dimnames[[1]]
  missing_features <- setdiff(train_features, present_features)
  
  # Create a matrix of zeros for these missing features
  zeros <- data.frame(
    matrix(data = 0, nrow = length(missing_features), ncol = ncol(betas), 
           dimnames = list(missing_features, colnames(betas))))
  
  # add back into the data
  dat_in <- t(rbind(dat, zeros)[train_features,])
  
  # run prediction
  preds <- as.data.frame(predict(glm_fit, dat_in, s = glm_fit$lambdaOpt, type = 'class'))
  probs <- as.data.frame(predict(glm_fit, dat_in, s = glm_fit$lambdaOpt, type = 'response'))
  p <- cbind(preds, probs)
  colnames(p) <- c('Predicted_ethnicity_nothresh', paste0('Prob_', glm_fit$classnames))
  p$Highest_Prob <- apply(p[,2:4], 1, max)
  p$Predicted_ethnicity <- ifelse(p$Highest_Prob < threshold, 'Ambiguous', 
                                        as.character(p$Predicted_ethnicity_nothresh))
  p$Sample_ID <- rownames(p)
  p <- p[,c(7,1,6, 2:5)]
  
  return(p)
}

#now compare:
pl_infer_ethnicity(newDat)
pred_prob
```

yay

# Save data

saveRDS(glm_fit, '../../Robjects_final/05_glm_fit.rds')

For south asian samples amy is using:

```{r}
# add predictions to pDat
pDat_SA <- pDat_SA %>% left_join(pred_prob, by = c('sampleNames' = 'Sample_ID'))

# combine with other samples pDat
pDat_final <- readRDS('../../Robjects_final/03_final_pData.rds')
pDat_save <- pDat_final %>% 
  bind_rows(pDat_SA %>% select(-matAge, -CH, -GA_2) %>% mutate(GA = as.numeric(GA)) %>%
              rename(Self_reported_ethnicity = Ethnicity, Cohort_owner = Dataset))
dim(pDat_save) # 506 33

# recall ambiguous samples based on 0.75 threshold
pDat_save$Predicted_ethnicity <- ifelse(pDat_save$Highest_Prob < 0.75, 'Ambiguous', 
                                        as.character(pDat_save$Predicted_ethnicity))
table(pDat_save$Predicted_ethnicity)
saveRDS(pDat_save, '../../Robjects_final/05_pDat_506samps.rds')

ggplot(pDat_save, aes(x = Prob_Caucasian, y = Prob_African, col = Predicted_ethnicity)) +
  geom_point()

ggplot(pDat_save, aes(x = Prob_Caucasian, y = Prob_African, col = Self_reported_ethnicity)) +
  geom_point()
```